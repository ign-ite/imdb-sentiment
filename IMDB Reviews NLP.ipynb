{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of numerical features : \n",
      "                                                    review sentiment\n",
      "count                                               50000     50000\n",
      "unique                                              49582         2\n",
      "top     Loved today's show!!! It was a variety and not...  positive\n",
      "freq                                                    5     25000\n",
      "=======================================================================\n",
      "\n",
      "Total number of reviews:  50000\n",
      "=======================================================================\n",
      "\n",
      "Total number of Sentiments:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      One of the other reviewers has mentioned that ...          1\n",
       "1      A wonderful little production. <br /><br />The...          1\n",
       "2      I thought this was a wonderful way to spend ti...          1\n",
       "3      Basically there's a family where a little boy ...          0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Summary statistics of numerical features : \\n\", df.describe())\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "print(\"\\nTotal number of reviews: \",len(df))\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "\n",
    "print(\"\\nTotal number of Sentiments: \", len(list(set(df['sentiment']))))\n",
    "\n",
    "df['sentiment'] = np.where(df['sentiment'] == \"positive\", 1, 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "# sns.countplot(df['Rating'])\n",
    "df['sentiment'].value_counts().sort_index().plot(kind='bar',color = 'blue')\n",
    "plt.title('Distribution of Rating')\n",
    "plt.grid()\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "ax = plt.axes()\n",
    "ax.set_facecolor(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17259</th>\n",
       "      <td>It's amazing that from a good, though not wond...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44995</th>\n",
       "      <td>I watched this movie for the first time a few ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35249</th>\n",
       "      <td>When I saw this movie first, it was long ago o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29535</th>\n",
       "      <td>Sure it was well shot and made, very well shot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28607</th>\n",
       "      <td>It was an interesting and entertaining movie w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23419</th>\n",
       "      <td>What can I say? An excellent end to an excelle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25611</th>\n",
       "      <td>DarkWolf tells the tale of a young waitress na...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>Oh dear, just what we need another Essex -Cock...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42838</th>\n",
       "      <td>I was looking forward to seeing this movie aft...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19642</th>\n",
       "      <td>A bright youngster interested in \"serious\" mus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "17259  It's amazing that from a good, though not wond...          1\n",
       "44995  I watched this movie for the first time a few ...          0\n",
       "35249  When I saw this movie first, it was long ago o...          1\n",
       "29535  Sure it was well shot and made, very well shot...          1\n",
       "28607  It was an interesting and entertaining movie w...          1\n",
       "...                                                  ...        ...\n",
       "23419  What can I say? An excellent end to an excelle...          1\n",
       "25611  DarkWolf tells the tale of a young waitress na...          0\n",
       "3602   Oh dear, just what we need another Essex -Cock...          0\n",
       "42838  I was looking forward to seeing this movie aft...          0\n",
       "19642  A bright youngster interested in \"serious\" mus...          1\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=0.1, random_state=0) #uncomment to use full set of data\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 450 training examples and 50 validation examples. \n",
      "\n",
      "Show a review in the training set : \n",
      " In 1968 when, \"SYMBIOPSYCHOTAXIPLASM: Take One\", was released, it came from out of nowhere, and struck like a psychedelic thunder bolt. Afro-American actor and film maker, William Greaves, aimed to forever alter the 'news-reel' style of documentary film-making, and to this day, there has never been anything quite like it. The movie is a film about 'the making of a film', and intentionally written and directed so as to create as much controversy and contradiction as possible. Set in New York's Central Park, the action and scant dialog concern a couple who fight and bicker about homosexuality and abortion. The woman wants out of the relationship, and the man wants an explanation. Near the end of this interaction, a drunk homeless man interrupts the proceedings and offers his commentary, and personal back-story. Then, after the principle footage has been shot, the film crew add their own views of the film-maker and what they feel is his inept handling of the movie. And during the entire film, multiple cameras are employed to record the action within the scene, and extraneous commentary by cast, crew, and onlookers. I would certainly recommend this film to anyone who has an interest in Avant Garde film makers such as Andy Warhol, John Cassavetes, or Jim Jarmusch. William Greaves attempts to show that a thing cannot be truly observed and understood because the viewing itself would alter the reality. \"SYMBIOPSYCHOTAXIPLASM: Take One\" can be seen as a cinematic representation or application of The Uncertainty Principle. This is only one possible explanation, and Greave's true intent is certainly open for speculation. Above all else, this film seeks to confound, confront. and stimulate, and without a doubt, succeeds admirably.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19295    It was great to see some of my favorite stars ...\n",
       " 47415    Despite the feelings of most \"Star Wars\" fans,...\n",
       " 23491    Whoopi was the only reason I watched the Oscar...\n",
       " 45579    Darr was a Super Hit film, which was loved by ...\n",
       " 4079     Ah, I loved this movie. I think it had it all....\n",
       "                                ...                        \n",
       " 47060    Yikes did this movie blow. The characters were...\n",
       " 32942    If you've ever listened to any of the James Le...\n",
       " 39292    This is by far the worst Hemingway adaptation ...\n",
       " 30971    I can remember this movie from when i was a sm...\n",
       " 26150    I didn't like Underdog!I mean it was really un...\n",
       " Name: review, Length: 450, dtype: object,\n",
       " 19295    0\n",
       " 47415    1\n",
       " 23491    1\n",
       " 45579    1\n",
       " 4079     1\n",
       "         ..\n",
       " 47060    0\n",
       " 32942    0\n",
       " 39292    0\n",
       " 30971    1\n",
       " 26150    0\n",
       " Name: sentiment, Length: 450, dtype: int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], \\\n",
    "                                                    test_size=0.1, random_state=0)\n",
    "\n",
    "print('Load %d training examples and %d validation examples. \\n' %(X_train.shape[0],X_test.shape[0]))\n",
    "print('Show a review in the training set : \\n', X_train.iloc[10])\n",
    "X_train,y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "<br>\n",
    "\n",
    "**Step 1 : Preprocess raw reviews to cleaned reviews**\n",
    "\n",
    "**Step 2 : Create BoW using CountVectorizer / Tfidfvectorizer in sklearn**\n",
    "\n",
    "**Step 3 : Transform review text to numerical representations (feature vectors)**\n",
    "\n",
    "**Step 4 : Fit feature vectors to supervised learning algorithm (eg. Naive Bayes, Logistic regression, etc.)**\n",
    "\n",
    "**Step 5 : Improve the model performance by GridSearch**\n",
    "\n",
    "# Text Preprocessing\n",
    "<br>\n",
    "\n",
    "**Step 1 : remove html tags using BeautifulSoup**\n",
    "\n",
    "**Step 2 : remove non-character such as digits and symbols**\n",
    "\n",
    "**Step 3 : convert to lower case**\n",
    "\n",
    "**Step 4 : remove stop words such as \"the\" and \"and\" if needed**\n",
    "\n",
    "**Step 5 : convert to root words by stemming if needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'html.parser').get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    words = letters_only.lower().split() \n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True:\n",
    "\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup \n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word2vec\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from bs4 import BeautifulSoup \n",
    "import logging\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\n",
    "    \n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer with Mulinomial Naive Bayes (Benchmark Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BernoulliNB, MultinomialNB\n\u001b[0;32m      3\u001b[0m countVect \u001b[38;5;241m=\u001b[39m CountVectorizer() \n\u001b[1;32m----> 4\u001b[0m X_train_countVect \u001b[38;5;241m=\u001b[39m countVect\u001b[38;5;241m.\u001b[39mfit_transform(X_train_cleaned)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features : \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\u001b[38;5;28mlen\u001b[39m(countVect\u001b[38;5;241m.\u001b[39mget_feature_names())) \u001b[38;5;66;03m#6378 \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow some feature names : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, countVect\u001b[38;5;241m.\u001b[39mget_feature_names()[::\u001b[38;5;241m1000\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "countVect = CountVectorizer() \n",
    "X_train_countVect = countVect.fit_transform(X_train_cleaned)\n",
    "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #6378 \n",
    "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])\n",
    "\n",
    "\n",
    "# Train MultinomialNB classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(countVect,open('countVect_imdb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "def modelEvaluation(predictions):\n",
    "    '''\n",
    "    Print model evaluation to predicted result \n",
    "    '''\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8140\n",
      "\n",
      "AUC score : 0.8142\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       249\n",
      "           1       0.85      0.77      0.81       251\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.82      0.81      0.81       500\n",
      "weighted avg       0.82      0.81      0.81       500\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[214  35]\n",
      " [ 58 193]]\n"
     ]
    }
   ],
   "source": [
    "predictions = mnb.predict(countVect.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(mnb,open('Naive_Bayes_model_imdb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 10505 \n",
      "\n",
      "Show some feature names : \n",
      " ['00', 'belonged', 'completion', 'dubious', 'garbage', 'interviewing', 'million', 'plays', 'rough', 'strike', 'vein']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features with smallest coefficients :\n",
      "['bad' 'worst' 'awful' 'no' 'waste' 'poor' 'terrible' 'boring' 'even'\n",
      " 'minutes']\n",
      "\n",
      "Top 10 features with largest coefficients : \n",
      "['great' 'and' 'excellent' 'best' 'it' 'wonderful' 'very' 'also' 'well'\n",
      " 'love']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8500\n",
      "\n",
      "AUC score : 0.8500\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       249\n",
      "           1       0.85      0.85      0.85       251\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.85      0.85      0.85       500\n",
      "weighted avg       0.85      0.85      0.85       500\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[211  38]\n",
      " [ 37 214]]\n"
     ]
    }
   ],
   "source": [
    "predictions = lr.predict(tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best paramenter set is : \n",
      " {'lr__C': 10, 'tfidf__max_features': None, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
      "\n",
      "Accuracy on validation set: 0.8720\n",
      "\n",
      "AUC score : 0.8720\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       249\n",
      "           1       0.87      0.88      0.87       251\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[216  33]\n",
      " [ 31 220]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "\n",
    "params = {\"lr__C\":[0.1, 1, 10], \n",
    "          \"tfidf__min_df\": [1, 3], \n",
    "          \"tfidf__max_features\": [1000, None], \n",
    "          \"tfidf__ngram_range\": [(1,1), (1,2)], \n",
    "          \"tfidf__stop_words\": [None, \"english\"]} \n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid.fit(X_train_cleaned, y_train)\n",
    "print(\"The best paramenter set is : \\n\", grid.best_params_)\n",
    "\n",
    "\n",
    "# Evaluate on the validaton set\n",
    "predictions = grid.predict(X_test_cleaned)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "<br>\n",
    "\n",
    "**Step 1 : Parse review text to sentences (Word2Vec model takes a list of sentences as inputs)**\n",
    "\n",
    "**Step 2 : Create volcabulary list using Word2Vec model.**\n",
    "\n",
    "**Step 3 : Transform each review into numerical representation by computing average feature vectors of words therein.**\n",
    "\n",
    "**Step 4 : Fit the average feature vectors to Random Forest Classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['the', 'crimson', 'rivers', 'is', 'one', 'of', 'the', 'most', 'over', 'directed', 'over', 'the', 'top', 'over', 'everything', 'mess', 'i', 've', 'ever', 'seen', 'come', 'out', 'of', 'france', 'there', 's', 'nothing', 'worse', 'than', 'a', 'french', 'production', 'trying', 'to', 'out', 'do', 'films', 'made', 'in', 'hollywood', 'and', 'cr', 'is', 'a', 'perfect', 'example', 'of', 'such', 'a', 'wannabe', 'horror', 'action', 'buddy', 'flick', 'i', 'almost', 'stopped', 'it', 'halfway', 'through', 'because', 'i', 'knew', 'it', 'wouldn', 't', 'amount', 'to', 'anything', 'but', 'french', 'guys', 'trying', 'to', 'show', 'off', 'the', 'film', 'starts', 'off', 'promisingly', 'like', 'some', 'sort', 'of', 'expansive', 'horror', 'film', 'but', 'it', 'quickly', 'shifts', 'genres', 'from', 'horror', 'to', 'action', 'to', 'x', 'files', 'type', 'to', 'buddy', 'flick', 'that', 'in', 'the', 'end', 'cr', 'is', 'all', 'of', 'it', 'and', 'also', 'none', 'of', 'it', 'it', 's', 'so', 'full', 'of', 'clich', 's', 'that', 'at', 'one', 'point', 'i', 'thought', 'the', 'whole', 'thing', 'was', 'a', 'comedy', 'the', 'painful', 'dialogue', 'and', 'those', 'silent', 'pauses', 'with', 'fades', 'outs', 'and', 'fades', 'ins', 'just', 'at', 'the', 'right', 'expositionary', 'moments', 'made', 'me', 'groan', 'i', 'thought', 'only', 'films', 'made', 'in', 'hollywood', 'used', 'this', 'hackneyed', 'technique', 'the', 'chase', 'scene', 'with', 'vincent', 'cassel', 'running', 'after', 'the', 'killer', 'is', 'so', 'over', 'directed', 'and', 'over', 'done', 'that', 'it', 's', 'almost', 'a', 'thing', 'of', 'beauty', 'the', 'climax', 'on', 'top', 'of', 'the', 'mountain', 'with', 'the', 'stupid', 'revelation', 'about', 'the', 'killer', 's', 'with', 'cassel', 'and', 'reno', 'playing', 'buddies', 'like', 'nolte', 'and', 'murphy', 'in', 'hrs', 'completely', 'derailed', 'what', 'little', 'credibility', 'the', 'film', 'had', 'by', 'then', 'it', 's', 'difficult', 'to', 'believe', 'that', 'the', 'director', 'of', 'the', 'crimson', 'rivers', 'also', 'directed', 'gothika', 'which', 'though', 'had', 'its', 'share', 'of', 'problems', 'doesn', 't', 'even', 'come', 'close', 'to', 'the', 'awfulness', 'of', 'this', 'overbaked', 'confused', 'film']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\n",
    "\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Parse each review in the training set into sentences\n",
    "sentences = []\n",
    "for review in X_train_cleaned:\n",
    "    sentences += parseSent(review, tokenizer,remove_stopwords=False)\n",
    "    \n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Volcabulary List usinhg Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model ...\n",
      "\n",
      "Number of words in the vocabulary list : 6945 \n",
      "\n",
      "Show first 10 words in the vocalbulary list  vocabulary list: \n",
      " ['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this']\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "num_features = 300  #embedding dimension                     \n",
    "min_word_count = 10                \n",
    "num_workers = 4       \n",
    "context = 10                                                                                          \n",
    "downsampling = 1e-3 \n",
    "\n",
    "print(\"Training Word2Vec model ...\\n\")\n",
    "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n",
    "                 window = context, sample = downsampling)\n",
    "w2v.init_sims(replace=True)\n",
    "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
    "\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(review, model, num_features):\n",
    "    '''\n",
    "    Transform a review to a feature vector by averaging feature vectors of words \n",
    "    appeared in that review and in the volcabulary list created\n",
    "    '''\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\n",
    "    isZeroVec = True\n",
    "    for word in review:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "            isZeroVec = False\n",
    "    if isZeroVec == False:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    '''\n",
    "    Transform all reviews to feature vectors using makeFeatureVec()\n",
    "    '''\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 4500 feature vectors with 300 dimensions\n",
      "Validation set : 500 feature vectors with 300 dimensions\n"
     ]
    }
   ],
   "source": [
    "X_train_cleaned = []\n",
    "for review in X_train:\n",
    "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\n",
    "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\n",
    "\n",
    "\n",
    "# Get feature vectors for validation set\n",
    "X_test_cleaned = []\n",
    "for review in X_test:\n",
    "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\n",
    "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set: 0.7640\n",
      "\n",
      "AUC score : 0.7641\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.77       249\n",
      "           1       0.77      0.75      0.76       251\n",
      "\n",
      "    accuracy                           0.76       500\n",
      "   macro avg       0.76      0.76      0.76       500\n",
      "weighted avg       0.76      0.76      0.76       500\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[194  55]\n",
      " [ 63 188]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(trainVector, y_train)\n",
    "predictions = rf.predict(testVector)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "<br>\n",
    "\n",
    "**Step 1 : Prepare X_train and X_test to 2D tensor.**\n",
    "    \n",
    "**Step 2 : Train a simple LSTM (embeddign layer => LSTM layer => dense layer).**\n",
    "    \n",
    "**Step 3 : Compile and fit the model using log loss function and ADAM optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4500, 200)\n",
      "========================================\n",
      "X_test shape: (500, 200)\n",
      "========================================\n",
      "y_train shape: (4500, 4)\n",
      "========================================\n",
      "y_test shape: (500, 4)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "top_words = 40000 \n",
    "maxlen = 200 \n",
    "batch_size = 62\n",
    "nb_classes = 4\n",
    "nb_epoch = 6\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq.shape)\n",
    "print(\"========================================\")\n",
    "print('X_test shape:', X_test_seq.shape)\n",
    "print(\"========================================\")\n",
    "print('y_train shape:', y_train_seq.shape)\n",
    "print(\"========================================\")\n",
    "print('y_test shape:', y_test_seq.shape)\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         5120000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 5,252,100\n",
      "Trainable params: 5,252,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, 128, dropout=0.2))\n",
    "model1.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model1.add(Dense(nb_classes))\n",
    "model1.add(Activation('softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "4500/4500 [==============================] - 22s 5ms/step - loss: 0.3760 - accuracy: 0.7594\n",
      "Epoch 2/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.2857 - accuracy: 0.8577\n",
      "Epoch 3/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.1591 - accuracy: 0.9347\n",
      "Epoch 4/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.0838 - accuracy: 0.9699\n",
      "Epoch 5/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.0385 - accuracy: 0.9874\n",
      "Epoch 6/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.0225 - accuracy: 0.9925\n",
      "500/500 [==============================] - 1s 1ms/step\n",
      "Test loss : 0.4559\n",
      "Test accuracy : 0.8750\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "# Model evluation\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 4500)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_seq),len(y_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of weight matrix in the embedding layer :  (40000, 128)\n",
      "Size of weight matrix in the hidden layer :  (128, 512)\n",
      "Size of weight matrix in the output layer :  (128, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of weight matrix in the embedding layer : \", \\\n",
    "      model1.layers[0].get_weights()[0].shape)\n",
    "\n",
    "# get weight matrix of the hidden layer\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\n",
    "      model1.layers[1].get_weights()[0].shape)\n",
    "\n",
    "# get weight matrix of the output layer\n",
    "print(\"Size of weight matrix in the output layer : \", \\\n",
    "      model1.layers[2].get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model1,open('model1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\n",
    "\n",
    "embedding_matrix = w2v.wv.syn0 \n",
    "print(\"Shape of embedding matrix : \", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = embedding_matrix.shape[0] #4016 \n",
    "maxlen = 300 \n",
    "batch_size = 62\n",
    "nb_classes = 4\n",
    "nb_epoch = 7\n",
    "\n",
    "\n",
    "# Vectorize X_train and X_test to 2D tensor\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_seq1 = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test_seq1 = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# one-hot encoding of y_train and y_test\n",
    "y_train_seq1 = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_seq1 = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "print('X_train shape:', X_train_seq1.shape)\n",
    "print(\"========================================\")\n",
    "print('X_test shape:', X_test_seq1.shape)\n",
    "print(\"========================================\")\n",
    "print('y_train shape:', y_train_seq1.shape)\n",
    "print(\"========================================\")\n",
    "print('y_test shape:', y_test_seq1.shape)\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_seq1),len(y_train_seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0], #4016\n",
    "                            embedding_matrix.shape[1], #300\n",
    "                            weights=[embedding_matrix])\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model2.add(Dense(nb_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(X_train_seq1, y_train_seq1, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
    "\n",
    "# Model evaluation\n",
    "score = model2.evaluate(X_test_seq1, y_test_seq1, batch_size=batch_size)\n",
    "print('Test loss : {:.4f}'.format(score[0]))\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of weight matrix in the embedding layer : \", \\\n",
    "      model2.layers[0].get_weights()[0].shape) \n",
    "\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\n",
    "      model2.layers[1].get_weights()[0].shape) \n",
    "\n",
    "print(\"Size of weight matrix in the output layer : \", \\\n",
    "      model2.layers[2].get_weights()[0].shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
